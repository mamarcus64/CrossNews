{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import copy\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "random.seed(8888)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold = json.load(open('data/crossnews_gold.json', 'r', encoding='utf-8'))\n",
    "silver = json.load(open('data/crossnews_silver.json', 'r', encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_authors = {}\n",
    "silver_authors = {}\n",
    "\n",
    "for docs, authors in [\n",
    "    (gold, gold_authors),\n",
    "    (silver, silver_authors)\n",
    "]:\n",
    "    for doc in docs:\n",
    "        author = doc['author']\n",
    "        authors[author] = authors.get(author, []) + [doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167404 337\n",
      "1202716 5665\n"
     ]
    }
   ],
   "source": [
    "print(len(gold), len(gold_authors))\n",
    "print(len(silver), len(silver_authors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_thresholds(authors, threshold, genre=None, filter=None):\n",
    "    result = set()\n",
    "    if filter is None:\n",
    "        filter = lambda x: True\n",
    "    for author_id, author_docs in authors.items():\n",
    "        doc_num = sum([1 if filter(doc) and (genre is None or doc['genre'] == genre) else 0 for doc in author_docs])\n",
    "        if doc_num >= threshold:\n",
    "            result.add(author_id)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_example(authors, author_id, genre=None):\n",
    "    author_docs = [doc for doc in authors[author_id] if genre is None or doc['genre'] == genre]\n",
    "    doc = random.choice(author_docs)\n",
    "    print('Document genre:', doc['genre'])\n",
    "    print('Document length:', len(doc['text']))\n",
    "    print(doc['text'][:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "237 5614\n"
     ]
    }
   ],
   "source": [
    "def length_filter(doc, length=100):\n",
    "    return len(doc['text']) >= length\n",
    "\n",
    "gold_filtered = doc_thresholds(gold_authors, 100, genre='Tweet', filter=length_filter)\n",
    "silver_filtered = doc_thresholds(silver_authors, 1, genre='Article', filter=length_filter)\n",
    "print(len(gold_filtered), len(silver_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_stats(values):\n",
    "    return f'total: {round(sum(values), 0)} count: {len(values)} mean: {round(np.mean(values), 3)} quartiles: {round(np.percentile(values, 25), 3)}/{round(np.percentile(values, 50), 3)}/{round(np.percentile(values, 75), 3)} std: {round(np.std(values), 3)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_author_statistics(authors):\n",
    "    print(f'Number of authors: {len(authors)}')\n",
    "    articles, tweets = [], []\n",
    "    for author in authors.values():\n",
    "        if len([len(doc['text']) for doc in author if doc['genre'] == 'Article']) > 0:\n",
    "            articles.append([len(doc['text']) for doc in author if doc['genre'] == 'Article'])\n",
    "        if len([len(doc['text']) for doc in author if doc['genre'] == 'Tweet']) > 0:\n",
    "            tweets.append([len(doc['text']) for doc in author if doc['genre'] == 'Tweet'])\n",
    "        \n",
    "    print('Articles per author: ' + list_stats([len(x) for x in articles]))\n",
    "    print('Chars per article per author: ' + list_stats([sum(x) / len(x) if len(x) > 0 else 0 for x in articles]))\n",
    "    print('Tweets per author: ' + list_stats([len(x) for x in tweets]))\n",
    "    print('Chars per tweet per author: ' + list_stats([sum(x) / len(x) if len(x) > 0 else 0 for x in tweets]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of authors: 337\n",
      "Articles per author: total: 58563 count: 337 mean: 173.777 quartiles: 100.0/100.0/250.0 std: 160.576\n",
      "Chars per article per author: total: 1149009.0 count: 337 mean: 3409.523 quartiles: 267.45/374.27/5753.864 std: 4751.575\n",
      "Tweets per author: total: 108841 count: 337 mean: 322.97 quartiles: 100.0/100.0/600.0 std: 260.896\n",
      "Chars per tweet per author: total: 50401.0 count: 337 mean: 149.558 quartiles: 94.492/141.16/211.67 std: 59.551\n"
     ]
    }
   ],
   "source": [
    "print_author_statistics(gold_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_documents(authors, threshold, upper_char_limit=2500, method='random', is_test=False):\n",
    "    authors = copy.deepcopy(authors)\n",
    "    new_authors = {author: [] for author in authors.keys()}\n",
    "    for author, old_docs in authors.items():\n",
    "        articles = [doc for doc in old_docs if doc['genre'] == 'Article']\n",
    "        tweets = [doc for doc in old_docs if doc['genre'] == 'Tweet']\n",
    "        \n",
    "        for docs in [articles, tweets]:\n",
    "            if method == 'random':\n",
    "                random.shuffle(docs)\n",
    "            elif method == 'greedy':\n",
    "                docs = sorted(docs, key=lambda x: len(x['text']), reverse=True)\n",
    "            new_doc = None\n",
    "            for doc in docs:\n",
    "                if new_doc is None:\n",
    "                    new_doc = doc\n",
    "                else:\n",
    "                    new_doc['text'] += f'<new> {doc[\"text\"]}'\n",
    "                if len(new_doc['text']) >= threshold:\n",
    "                    text = new_doc['text']\n",
    "                    # only want to create multiple pairs from same datum in train setup\n",
    "                    for i in range(0, max(len(text) // upper_char_limit, 1) if not is_test else 1):\n",
    "                        new_doc['text'] = text[upper_char_limit*i:upper_char_limit*(i+1)]\n",
    "                        if len(new_doc['text']) >= threshold:\n",
    "                            new_authors[author].append(copy.deepcopy(new_doc))\n",
    "                    new_doc = None\n",
    "    return new_authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of authors: 5665\n",
      "Articles per author: total: 158984 count: 5614 mean: 28.319 quartiles: 2.0/4.0/17.0 std: 112.14\n",
      "Chars per article per author: total: 42858989.0 count: 5614 mean: 7634.305 quartiles: 3158.0/5503.0/8810.329 std: 8183.977\n",
      "Tweets per author: total: 1043732 count: 1925 mean: 542.198 quartiles: 500.0/600.0/600.0 std: 371.168\n",
      "Chars per tweet per author: total: 236446.0 count: 1925 mean: 122.829 quartiles: 91.49/116.665/149.153 std: 42.284\n"
     ]
    }
   ],
   "source": [
    "print_author_statistics(silver_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of authors: 5665\n",
      "Articles per author: total: 355047 count: 5610 mean: 63.288 quartiles: 3.0/12.0/43.0 std: 216.016\n",
      "Chars per article per author: total: 12344966.0 count: 5610 mean: 2200.529 quartiles: 2194.083/2481.302/2500.0 std: 530.454\n",
      "Tweets per author: total: 237174 count: 1900 mean: 124.828 quartiles: 80.0/117.0/153.25 std: 101.02\n",
      "Chars per tweet per author: total: 1078316.0 count: 1900 mean: 567.535 quartiles: 559.752/568.952/576.468 std: 13.842\n"
     ]
    }
   ],
   "source": [
    "print_author_statistics(stack_documents(silver_authors, 500, method='greedy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of authors: 337\n",
      "Articles per author: total: 58563 count: 337 mean: 173.777 quartiles: 100.0/100.0/250.0 std: 160.576\n",
      "Chars per article per author: total: 1149009.0 count: 337 mean: 3409.523 quartiles: 267.45/374.27/5753.864 std: 4751.575\n",
      "Tweets per author: total: 108841 count: 337 mean: 322.97 quartiles: 100.0/100.0/600.0 std: 260.896\n",
      "Chars per tweet per author: total: 50401.0 count: 337 mean: 149.558 quartiles: 94.492/141.16/211.67 std: 59.551\n"
     ]
    }
   ],
   "source": [
    "print_author_statistics(gold_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of authors: 337\n",
      "Articles per author: total: 48219 count: 337 mean: 143.083 quartiles: 43.0/54.0/249.0 std: 175.025\n",
      "Chars per article per author: total: 490332.0 count: 337 mean: 1454.992 quartiles: 625.6/690.919/2476.576 std: 893.271\n",
      "Tweets per author: total: 25228 count: 337 mean: 74.861 quartiles: 34.0/41.0/115.0 std: 59.273\n",
      "Chars per tweet per author: total: 194301.0 count: 337 mean: 576.561 quartiles: 564.259/575.409/588.7 std: 18.603\n"
     ]
    }
   ],
   "source": [
    "print_author_statistics(stack_documents(gold_authors, 500, method='greedy', is_test=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document genre: Tweet\n",
      "Document length: 79\n",
      "<PERSON> Thank you <PERSON>. Pretentious, yes. But also not a whole lot of fun.\n",
      "Document genre: Article\n",
      "Document length: 888\n",
      "TREVOR PHILLIPS\n",
      "Dark times call for plain speaking not platitudes\n",
      "Ukrainians show us that sometimes offence is the best response when democracy is threatened\n",
      "The Times\n",
      "That thing about never meeting your heroes is mostly nonsense. I blame Proust’s À La Recherche du Temps Perdu (a book more quoted fr\n"
     ]
    }
   ],
   "source": [
    "print_example(gold_authors, random.choice(tuple(gold_filtered)), genre='Tweet')\n",
    "print_example(silver_authors, random.choice(tuple(silver_filtered)), genre='Article')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_golds = stack_documents(gold_authors, 500, method='greedy', is_test=True)\n",
    "stacked_silvers = stack_documents(silver_authors, 500, method='greedy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_verification_pairs(data, first_genre, second_genre):\n",
    "    data = copy.deepcopy(data)\n",
    "    first_docs = {\n",
    "        author_name: [doc for doc in author_docs if doc['genre'] == first_genre]\n",
    "            for author_name, author_docs in data.items()\n",
    "    }\n",
    "    if first_genre == second_genre:\n",
    "        second_docs = first_docs\n",
    "    else:\n",
    "        second_docs = {\n",
    "            author_name: [doc for doc in author_docs if doc['genre'] == second_genre]\n",
    "                for author_name, author_docs in data.items()\n",
    "        }\n",
    "        \n",
    "    delete_threshold = 0 if first_genre != second_genre else 1\n",
    "        \n",
    "    for author in data.keys():\n",
    "        random.shuffle(first_docs[author])\n",
    "        random.shuffle(second_docs[author])\n",
    "        \n",
    "        if len(first_docs[author]) <= delete_threshold:\n",
    "            del first_docs[author]\n",
    "        if len(second_docs[author]) <= delete_threshold:\n",
    "            del second_docs[author]\n",
    "    \n",
    "    # each pair is a 3-tuple of (label, first_text, second_text), where label == 0 if different authors and 1 if same author\n",
    "    pairs = []\n",
    "   \n",
    "    next_pick = 'same'\n",
    "    # need at least two authors for each genre to pick pairs\n",
    "    while len(first_docs) > 1 and len(second_docs) > 1:\n",
    "        # pick authors for next pair\n",
    "        first_author_pool = list(first_docs.keys())\n",
    "        second_author_pool = list(second_docs.keys())\n",
    "        if next_pick == 'diff':\n",
    "            first_author = random.choice(first_author_pool)\n",
    "            second_author = random.choice(second_author_pool)\n",
    "            while first_author == second_author:\n",
    "                second_author = random.choice(second_author_pool)\n",
    "        elif next_pick == 'same':\n",
    "            first_author = random.choice(first_author_pool)\n",
    "            # try picking 10 random authors, if this doesn't work, then iterate through all authors\n",
    "            if first_author not in second_author_pool:\n",
    "                for _ in range(10):\n",
    "                    first_author = random.choice(first_author_pool)\n",
    "                    if first_author in second_author_pool:\n",
    "                        break\n",
    "            if first_author not in second_author_pool:\n",
    "                random.shuffle(first_author_pool)\n",
    "                found_same_author = False\n",
    "                for author in first_author_pool:\n",
    "                    if author in second_author_pool:\n",
    "                        first_author = author\n",
    "                        found_same_author = True\n",
    "                        break\n",
    "                if not found_same_author:\n",
    "                    break # bailing, no same authors left\n",
    "            second_author = first_author\n",
    "        \n",
    "        if len(first_docs[first_author]) == 0 or len(second_docs[second_author]) == 0:\n",
    "            print(first_author, second_author, len(first_docs[first_author]), len(second_docs[second_author]))\n",
    "        pairs.append((1 if next_pick == 'same' else 0, first_docs[first_author].pop()['text'], second_docs[second_author].pop()['text'], first_author, second_author))\n",
    "        \n",
    "        if len(first_docs[first_author]) <= delete_threshold:\n",
    "            del first_docs[first_author]\n",
    "        if (first_genre != second_genre or first_author != second_author) and len(second_docs[second_author]) <= delete_threshold:\n",
    "            del second_docs[second_author]\n",
    "                \n",
    "        # alternate pair type\n",
    "        next_pick = 'diff' if next_pick == 'same' else 'same'\n",
    "\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_pair_stats(pairs, save=None):\n",
    "    print(f'Total pairs: {len(pairs)}; same-pair percent: {sum([pair[0] for pair in pairs]) / len(pairs)}')\n",
    "    \n",
    "    authors = set()\n",
    "    first_genre_lengths, second_genre_lengths = [], []\n",
    "    for pair in pairs:\n",
    "        first_length, second_length = len(pair[1]), len(pair[2])\n",
    "        first_author, second_author = pair[3], pair[4]\n",
    "        authors.add(first_author)\n",
    "        authors.add(second_author)\n",
    "        first_genre_lengths.append(first_length)\n",
    "        second_genre_lengths.append(second_length)\n",
    "    \n",
    "    print(f'Num authors: {len(authors)}')\n",
    "    print(f'Avg. chars per first genre: {sum(first_genre_lengths) / len(first_genre_lengths)}')\n",
    "    print(f'Avg. chars per second genre: {sum(second_genre_lengths) / len(second_genre_lengths)}')\n",
    "    \n",
    "    if save:\n",
    "        print(f'Saving to {save}.')\n",
    "        columns = ['label', 'text0', 'text1']\n",
    "        df = pd.DataFrame([(pair[0], pair[1], pair[2]) for pair in pairs], columns=columns)\n",
    "        df.to_csv(save, index=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pairs_Article_X = create_verification_pairs(stacked_golds, 'Article', 'Tweet')\n",
    "test_pairs_Article_Article = create_verification_pairs(stacked_golds, 'Article', 'Article')\n",
    "test_pairs_X_X = create_verification_pairs(stacked_golds, 'Tweet', 'Tweet')\n",
    "train_pairs = create_verification_pairs(stacked_silvers, 'Article', 'Tweet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pairs: 25124; same-pair percent: 0.5\n",
      "Num authors: 337\n",
      "Avg. chars per first genre: 1843.932375417927\n",
      "Avg. chars per second genre: 574.1184126731412\n",
      "Saving to pairs/test_Article_X.csv.\n"
     ]
    }
   ],
   "source": [
    "os.makedirs('pairs', exist_ok=True)\n",
    "print_pair_stats(test_pairs_Article_X, save='pairs/test_Article_X.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pairs: 22952; same-pair percent: 0.5\n",
      "Num authors: 337\n",
      "Avg. chars per first genre: 2101.9971244336007\n",
      "Avg. chars per second genre: 2100.6907023353083\n",
      "Saving to pairs/test_Article_Article.csv.\n"
     ]
    }
   ],
   "source": [
    "print_pair_stats(test_pairs_Article_Article, save='pairs/test_Article_Article.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pairs: 12436; same-pair percent: 0.5\n",
      "Num authors: 337\n",
      "Avg. chars per first genre: 574.2765358636217\n",
      "Avg. chars per second genre: 573.853168221293\n",
      "Saving to pairs/test_X_X.csv.\n"
     ]
    }
   ],
   "source": [
    "print_pair_stats(test_pairs_X_X, save='pairs/test_X_X.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pairs: 104482; same-pair percent: 0.5\n",
      "Num authors: 5662\n",
      "Avg. chars per first genre: 2313.8631056067074\n",
      "Avg. chars per second genre: 569.3298175762333\n",
      "Saving to pairs/train_Article_X.csv.\n"
     ]
    }
   ],
   "source": [
    "print_pair_stats(train_pairs, save='pairs/train_Article_X.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
