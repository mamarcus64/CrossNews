{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import copy\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "random.seed(8888)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold = json.load(open('data/crossnews_gold.json', 'r', encoding='utf-8'))\n",
    "silver = json.load(open('data/crossnews_silver.json', 'r', encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_authors = {}\n",
    "silver_authors = {}\n",
    "\n",
    "for docs, authors in [\n",
    "    (gold, gold_authors),\n",
    "    (silver, silver_authors)\n",
    "]:\n",
    "    for doc in docs:\n",
    "        author = doc['author']\n",
    "        authors[author] = authors.get(author, []) + [doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221420 500\n",
      "1258077 2260\n"
     ]
    }
   ],
   "source": [
    "print(len(gold), len(gold_authors))\n",
    "print(len(silver), len(silver_authors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_stats(values):\n",
    "    return f'total: {round(sum(values), 0)} count: {len(values)} mean: {round(np.mean(values), 3)} quartiles: {round(np.percentile(values, 25), 3)}/{round(np.percentile(values, 50), 3)}/{round(np.percentile(values, 75), 3)} std: {round(np.std(values), 3)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_author_statistics(authors):\n",
    "    print(f'Number of authors: {len(authors)}')\n",
    "    articles, tweets = [], []\n",
    "    for author in authors.values():\n",
    "        if len([len(doc['text']) for doc in author if doc['genre'] == 'Article']) > 0:\n",
    "            articles.append([len(doc['text']) for doc in author if doc['genre'] == 'Article'])\n",
    "        if len([len(doc['text']) for doc in author if doc['genre'] == 'Tweet']) > 0:\n",
    "            tweets.append([len(doc['text']) for doc in author if doc['genre'] == 'Tweet'])\n",
    "        \n",
    "    print('Articles per author: ' + list_stats([len(x) for x in articles]))\n",
    "    print('Chars per article per author: ' + list_stats([sum(x) / len(x) if len(x) > 0 else 0 for x in articles]))\n",
    "    print('Tweets per author: ' + list_stats([len(x) for x in tweets]))\n",
    "    print('Chars per tweet per author: ' + list_stats([sum(x) / len(x) if len(x) > 0 else 0 for x in tweets]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_documents(authors, min_char_threshold, upper_char_limit=5000, method='random', is_train=False):\n",
    "    authors = copy.deepcopy(authors)\n",
    "    new_authors = {author: [] for author in authors.keys()}\n",
    "    for author, old_docs in authors.items():\n",
    "        articles = [doc for doc in old_docs if doc['genre'] == 'Article']\n",
    "        tweets = [doc for doc in old_docs if doc['genre'] == 'Tweet']\n",
    "        \n",
    "        for docs in [articles, tweets]:\n",
    "            if method == 'random':\n",
    "                random.shuffle(docs)\n",
    "            elif method == 'greedy':\n",
    "                docs = sorted(docs, key=lambda x: len(x['text']), reverse=True)\n",
    "            new_doc = None\n",
    "            for doc in docs:\n",
    "                if new_doc is None:\n",
    "                    new_doc = doc\n",
    "                else:\n",
    "                    new_doc['text'] += f' <new> {doc[\"text\"]}'\n",
    "                if len(new_doc['text']) >= min_char_threshold:\n",
    "                    text = new_doc['text']\n",
    "                    # if training data, can split long data into multiple upper_char_limit-sized documents\n",
    "                    for i in range(0, max(len(text) // upper_char_limit, 1) if is_train else 1):\n",
    "                        new_doc['text'] = text[upper_char_limit*i:upper_char_limit*(i+1)]\n",
    "                        if len(new_doc['text']) >= min_char_threshold:\n",
    "                            new_authors[author].append(copy.deepcopy(new_doc))\n",
    "                    new_doc = None\n",
    "    return new_authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of authors: 2260\n",
      "Articles per author: total: 107184 count: 2260 mean: 47.427 quartiles: 2.0/6.0/27.0 std: 191.738\n",
      "Chars per article per author: total: 15776807.0 count: 2260 mean: 6980.888 quartiles: 3698.479/5743.333/8416.25 std: 5959.486\n",
      "Tweets per author: total: 1150893 count: 2260 mean: 509.245 quartiles: 500.0/599.0/600.0 std: 303.773\n",
      "Chars per tweet per author: total: 303510.0 count: 2260 mean: 134.296 quartiles: 102.865/128.322/160.662 std: 42.124\n"
     ]
    }
   ],
   "source": [
    "print_author_statistics(silver_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of authors: 2260\n",
      "Articles per author: total: 136061 count: 2259 mean: 60.231 quartiles: 2.0/8.0/36.0 std: 231.529\n",
      "Chars per article per author: total: 8925082.0 count: 2259 mean: 3950.9 quartiles: 3304.036/4446.889/4938.791 std: 1198.798\n",
      "Tweets per author: total: 279677 count: 2235 mean: 125.135 quartiles: 88.0/121.0/156.5 std: 84.547\n",
      "Chars per tweet per author: total: 1276488.0 count: 2235 mean: 571.135 quartiles: 563.406/572.097/579.425 std: 13.599\n"
     ]
    }
   ],
   "source": [
    "print_author_statistics(stack_documents(silver_authors, 500, method='greedy', is_train=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of authors: 500\n",
      "Articles per author: total: 96743 count: 500 mean: 193.486 quartiles: 100.0/245.0/250.0 std: 136.588\n",
      "Chars per article per author: total: 1554626.0 count: 500 mean: 3109.252 quartiles: 298.642/2148.323/4298.092 std: 3960.874\n",
      "Tweets per author: total: 124677 count: 500 mean: 249.354 quartiles: 100.0/100.0/599.0 std: 238.977\n",
      "Chars per tweet per author: total: 79036.0 count: 500 mean: 158.072 quartiles: 110.446/148.33/206.921 std: 57.186\n"
     ]
    }
   ],
   "source": [
    "print_author_statistics(gold_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of authors: 500\n",
      "Articles per author: total: 86632 count: 500 mean: 173.264 quartiles: 46.75/242.0/249.0 std: 150.648\n",
      "Chars per article per author: total: 1147242.0 count: 500 mean: 2294.484 quartiles: 651.54/2152.378/3549.734 std: 1503.377\n",
      "Tweets per author: total: 31407 count: 500 mean: 62.814 quartiles: 24.0/37.5/96.25 std: 58.092\n",
      "Chars per tweet per author: total: 288086.0 count: 500 mean: 576.173 quartiles: 566.254/575.862/586.281 std: 15.773\n"
     ]
    }
   ],
   "source": [
    "print_author_statistics(stack_documents(gold_authors, 500, method='greedy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_golds = stack_documents(gold_authors, 500, method='greedy')\n",
    "stacked_silvers = stack_documents(silver_authors, 500, method='greedy', is_train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2260\n",
      "136061\n",
      "2260\n",
      "279677\n",
      "51878\n",
      "2234\n"
     ]
    }
   ],
   "source": [
    "ss = list(stacked_silvers.values())\n",
    "articles = [[doc['id'] for doc in auth_docs if doc['genre'] == 'Article'] for auth_docs in ss]\n",
    "print(len(articles))\n",
    "print(sum([len(x) for x in articles]))\n",
    "\n",
    "tweets = [[doc['id'] for doc in auth_docs if doc['genre'] == 'Tweet'] for auth_docs in ss]\n",
    "print(len(tweets))\n",
    "print(sum([len(x) for x in tweets]))\n",
    "\n",
    "a = [min(len(x), len(y), 100) for x, y in zip(articles, tweets)]\n",
    "print(sum(a))\n",
    "print(sum([1 if x >= 1 else 0 for x in a]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pair_ids(first_list, second_list, max_docs_per_author=100):\n",
    "    pairs = []\n",
    "    used_articles = []\n",
    "    used_tweets = []\n",
    "    id_to_auth = {}\n",
    "    \n",
    "    def to_pair_id(s1, s2):\n",
    "        same = 1 if id_to_auth[s1] == id_to_auth[s2] else 0\n",
    "        # return f'{same}_{s2}_{s1}' if int(s1) > int(s2) else f'{same}_{s1}_{s2}'\n",
    "        return f'{same}_{s1}_{s2}'\n",
    "\n",
    "    for auth in first_list.keys():\n",
    "        articles = first_list[auth]\n",
    "        tweets = second_list[auth]\n",
    "        for i in range(min(len(articles), len(tweets), max_docs_per_author)):\n",
    "            used_articles.append(articles[i])\n",
    "            used_tweets.append(tweets[i])\n",
    "            id_to_auth[articles[i]] = auth\n",
    "            id_to_auth[tweets[i]] = auth\n",
    "            pairs.append(to_pair_id(articles[i], tweets[i]))\n",
    "            \n",
    "    for _ in tqdm(range(len(used_articles))):\n",
    "        article = used_articles.pop()\n",
    "        tweet = random.choice(used_tweets)\n",
    "        while id_to_auth[tweet] == id_to_auth[article]:\n",
    "            tweet = random.choice(used_tweets)\n",
    "        used_tweets.remove(tweet)\n",
    "        pairs.append(to_pair_id(article, tweet))\n",
    "    \n",
    "    return pairs\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pair_entries(pair_ids, data):\n",
    "    pairs = []\n",
    "    id_to_doc = {}\n",
    "    for author_docs in data.values():\n",
    "        id_to_doc.update({str(doc['id']): doc for doc in author_docs})\n",
    "    \n",
    "    random.shuffle(pair_ids)\n",
    "    \n",
    "    for pair in pair_ids:\n",
    "        same, first_id, second_id = tuple(pair.split('_')[:3])\n",
    "        same = int(same)\n",
    "        a = id_to_doc[first_id]\n",
    "        b = id_to_doc[second_id]\n",
    "        pairs.append((same, a['text'], b['text'], a['author'], b['author']))\n",
    "    \n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_pair_stats(pairs, save=None):\n",
    "    print(f'Total pairs: {len(pairs)}; same-pair percent: {sum([pair[0] for pair in pairs]) / len(pairs)}')\n",
    "    \n",
    "    authors = set()\n",
    "    first_genre_lengths, second_genre_lengths = [], []\n",
    "    for pair in pairs:\n",
    "        first_length, second_length = len(pair[1]), len(pair[2])\n",
    "        first_author, second_author = pair[3], pair[4]\n",
    "        authors.add(first_author)\n",
    "        authors.add(second_author)\n",
    "        first_genre_lengths.append(first_length)\n",
    "        second_genre_lengths.append(second_length)\n",
    "    \n",
    "    print(f'Num authors: {len(authors)}')\n",
    "    print(f'Avg. chars per first genre: {sum(first_genre_lengths) / len(first_genre_lengths)}')\n",
    "    print(f'Avg. chars per second genre: {sum(second_genre_lengths) / len(second_genre_lengths)}')\n",
    "    \n",
    "    if save:\n",
    "        print(f'Saving to {save}.')\n",
    "        columns = ['label', 'text0', 'text1']\n",
    "        df = pd.DataFrame([(pair[0], pair[1], pair[2]) for pair in pairs], columns=columns)\n",
    "        df.to_csv(save, index=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_verification_pairs(data, first_genre, second_genre, seed, max_docs_per_author):\n",
    "    all_articles = {author: [str(doc['id']) for doc in docs if doc['genre'] == 'Article'] for author, docs in data.items()}\n",
    "    all_tweets = {author: [str(doc['id']) for doc in docs if doc['genre'] == 'Tweet'] for author, docs in data.items()}\n",
    "\n",
    "    random.seed(seed) # do all the shuffling here instead of during sample time\n",
    "    for auth in all_articles.keys():\n",
    "        random.shuffle(all_articles[auth])\n",
    "        random.shuffle(all_tweets[auth])\n",
    "    \n",
    "    if first_genre != second_genre: # Article & Tweet\n",
    "        first_docs = all_articles\n",
    "        second_docs = all_tweets\n",
    "    else: # either Article & Article or Tweet & Tweet\n",
    "        first_docs, second_docs = {}, {} # split all of the single genre into two distinct dicts\n",
    "        all_docs = all_articles if first_genre == 'Article' else all_tweets\n",
    "        for auth in all_docs.keys():\n",
    "            author_docs = all_docs[auth]\n",
    "            if len(author_docs) >= 2:\n",
    "                first_docs[auth] = author_docs[len(author_docs) // 2:]\n",
    "                second_docs[auth] = author_docs[:len(author_docs) // 2]\n",
    "                \n",
    "    pair_ids = generate_pair_ids(first_docs, second_docs, max_docs_per_author=max_docs_per_author)\n",
    "    pairs = get_pair_entries(pair_ids, data)\n",
    "    return pairs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [00:00<00:00, 171249.20it/s]\n",
      "100%|██████████| 2500/2500 [00:00<00:00, 202996.03it/s]\n",
      "100%|██████████| 2500/2500 [00:00<00:00, 154003.05it/s]\n",
      "100%|██████████| 51911/51911 [00:07<00:00, 7035.56it/s] \n",
      "100%|██████████| 38910/38910 [00:04<00:00, 9270.53it/s] \n",
      "100%|██████████| 131057/131057 [01:03<00:00, 2078.12it/s] \n"
     ]
    }
   ],
   "source": [
    "test_pairs_Article_Tweet = create_verification_pairs(stacked_golds, 'Article', 'Tweet', 111, 5)\n",
    "test_pairs_Article_Article = create_verification_pairs(stacked_golds, 'Article', 'Article', 222, 5)\n",
    "test_pairs_Tweet_Tweet = create_verification_pairs(stacked_golds, 'Tweet', 'Tweet', 333, 5)\n",
    "train_pairs_Article_Tweet = create_verification_pairs(stacked_silvers, 'Article', 'Tweet', 444, 100)\n",
    "train_pairs_Article_Article = create_verification_pairs(stacked_silvers, 'Article', 'Article', 555, 100)\n",
    "train_pairs_Tweet_Tweet = create_verification_pairs(stacked_silvers, 'Tweet', 'Tweet', 666, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pairs: 5000; same-pair percent: 0.5\n",
      "Num authors: 500\n",
      "Avg. chars per first genre: 2286.4376\n",
      "Avg. chars per second genre: 575.0016\n",
      "Saving to pairs/test_Article_Tweet.csv.\n"
     ]
    }
   ],
   "source": [
    "os.makedirs('pairs', exist_ok=True)\n",
    "print_pair_stats(test_pairs_Article_Tweet, save='pairs/test_Article_Tweet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pairs: 5000; same-pair percent: 0.5\n",
      "Num authors: 500\n",
      "Avg. chars per first genre: 2282.156\n",
      "Avg. chars per second genre: 2277.48\n",
      "Saving to pairs/test_Article_Article.csv.\n"
     ]
    }
   ],
   "source": [
    "print_pair_stats(test_pairs_Article_Article, save='pairs/test_Article_Article.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pairs: 5000; same-pair percent: 0.5\n",
      "Num authors: 500\n",
      "Avg. chars per first genre: 578.1928\n",
      "Avg. chars per second genre: 576.2844\n",
      "Saving to pairs/test_Tweet_Tweet.csv.\n"
     ]
    }
   ],
   "source": [
    "print_pair_stats(test_pairs_Tweet_Tweet, save='pairs/test_Tweet_Tweet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pairs: 103822; same-pair percent: 0.5\n",
      "Num authors: 2234\n",
      "Avg. chars per first genre: 4019.8569667315214\n",
      "Avg. chars per second genre: 571.5849627246633\n",
      "Saving to pairs/train_Article_Tweet.csv.\n"
     ]
    }
   ],
   "source": [
    "print_pair_stats(train_pairs_Article_Tweet, save='pairs/train_Article_Tweet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pairs: 77820; same-pair percent: 0.5\n",
      "Num authors: 1844\n",
      "Avg. chars per first genre: 4008.3671549730147\n",
      "Avg. chars per second genre: 4005.877101002313\n",
      "Saving to pairs/train_Article_Article.csv.\n"
     ]
    }
   ],
   "source": [
    "print_pair_stats(train_pairs_Article_Article, save='pairs/train_Article_Article.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pairs: 262114; same-pair percent: 0.5\n",
      "Num authors: 2220\n",
      "Avg. chars per first genre: 574.3434078301808\n",
      "Avg. chars per second genre: 574.414834766552\n",
      "Saving to pairs/train_Tweet_Tweet.csv.\n"
     ]
    }
   ],
   "source": [
    "print_pair_stats(train_pairs_Tweet_Tweet, save='pairs/train_Tweet_Tweet.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
